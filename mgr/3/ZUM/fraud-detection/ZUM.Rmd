---
title: "ZUM"
output:
  html_document:
    df_print: paged
---

## Pakiety

```{r message = FALSE}
library("tidyverse")
library("mlr3verse")
library("future")
library("progressr")
library("progress")
```

## Kod pomocniczy

```{r message = FALSE}
source("src/data_preparation.R")
```

## Konfiguracja

```{r}
plan(list("multisession", "sequential"))
handlers("progress")
```


## Wczytanie danych

```{r}
data <- read_csv("data/creditcard.csv", col_types = c("Class" = "factor"))
data
```

```{r}
# tymczasowo wybierzmy mniej danych żeby szybciej się liczyło
data <- data %>% group_by(Class) %>% slice_sample(n = 100, replace = TRUE) %>% ungroup()
```


## Koszt

```{r}
frequencies <- data %>%
  count(Class) %>%
  column_to_rownames("Class")
costs <- matrix(c(0, 1 / frequencies["1", "n"], 1 / frequencies["0", "n"], 0), nrow = 2)
dimnames(costs) <- list(predicted = c(1, 0), real = c(1, 0))
costs
```

## Miary ewaluacyjne

```{r}
measure <- msr("classif.costs", costs = costs)

f1 <- msr("classif.fbeta")
roc_auc <- msr("classif.auc")
prauc <- msr("classif.prauc")
recall <- msr("classif.recall")
precision <- msr("classif.precision")

measure
```

## Zadanie

```{r}
task <- as_task_classif(data, target = "Class")
task
```
## Wizualizacja danych {.tabset}

```{r, results='asis', echo = FALSE}
for (feature in task$feature_names) {
  cat("###", feature, "\n")
  suppressMessages(print(autoplot(task$clone()$select(c(feature)), type = "pairs")))
  plot.new()
  dev.off()
  cat("\n\n")
}
```

## Modele

### Baseline

```{r}
baseline <- lrn("classif.featureless", predict_type = "prob", predict_sets = c("train", "test"))
```


### K najbliższych sąsiadów

```{r}
knn <- AutoTuner$new(
  learner = lrn("classif.kknn", predict_type = "prob", predict_sets = c("train", "test")) %>>% po("threshold"),
  resampling = rsmp("cv", folds = 3),
  measure = measure,
  search_space = ps(
    classif.kknn.k	 = p_int(1, 30),
    classif.kknn.kernel	 = p_fct(c("rectangular", "epanechnikov", "inv", "gaussian")),
    threshold.thresholds = p_dbl(lower = 0, upper = 1)
  ),
  terminator = trm("stagnation"),
  tuner = tnr("random_search")
)
```

### Maszyna wektorów nośnych

```{r}
svm <- AutoTuner$new(
  learner = lrn("classif.ksvm", type = "C-svc", predict_type = "prob", predict_sets = c("train", "test")) 
             %>>% po("threshold"),
  resampling = rsmp("cv", folds = 3),
  measure = measure,
  search_space = ps(
    classif.ksvm.kernel = p_fct(c("rbfdot", "polydot", "vanilladot")),
    classif.ksvm.sigma = p_dbl(0.01, 100, depends = classif.ksvm.kernel %in% c("rbfdot")),
    classif.ksvm.scale = p_dbl(0.01, 100, depends = classif.ksvm.kernel %in% c("polydot")),
    classif.ksvm.C = p_dbl(1, 100),
    threshold.thresholds = p_dbl(lower = 0, upper = 1)
  ),
  terminator = trm("stagnation"),
  tuner = tnr("random_search")
)
```

### Regresja logistyczna

```{r}
logreg <- 
logreg <- AutoTuner$new(
  learner = lrn("classif.log_reg", predict_type = "prob", predict_sets = c("train", "test")) %>>% po("threshold"),
  resampling = rsmp("cv", folds = 3),
  measure = measure,
  search_space = ps(
    threshold.thresholds = p_dbl(lower = 0, upper = 1)
  ),
  terminator = trm("stagnation"),
  tuner = tnr("random_search")
)
```

### Drzewo decyzyjne

```{r}
rpart <- AutoTuner$new(
  learner = lrn("classif.rpart", predict_type = "prob", predict_sets = c("train", "test")) %>>% po("threshold"),
  resampling = rsmp("cv", folds = 3),
  measure = measure,
  search_space = ps(
    classif.rpart.cp = p_dbl(0, 1),
    classif.rpart.maxdepth = p_int(1, 30),
    classif.rpart.minbucket = p_int(1, 100),
    threshold.thresholds = p_dbl(lower = 0, upper = 1)
  ),
  terminator = trm("stagnation"),
  tuner = tnr("random_search")
)
```

### Las losowy

```{r}
random_forest <- AutoTuner$new(
  learner = lrn("classif.ranger", predict_type = "prob", predict_sets = c("train", "test")) %>>% po("threshold"),
  resampling = rsmp("cv", folds = 3),
  measure = measure,
  search_space = ps(
    classif.ranger.num.trees = p_int(10, 1000),
    classif.ranger.max.depth = p_int(1, 30),
    classif.ranger.mtry = p_int(1, 30),
    classif.ranger.min.node.size = p_int(1, 10),
    threshold.thresholds = p_dbl(lower = 0, upper = 1)
  ),
  terminator = trm("stagnation"),
  tuner = tnr("random_search")
)
```

## Benchmark

```{r}
learners <- list(baseline, knn, svm, logreg, rpart, random_forest)
```

```{r}
design <- benchmark_grid(
  tasks = task,
  learners = learners,
  resamplings = rsmp("cv", folds = 5)
)
design
```

```{r attr.output='style="max-height: 500px;"'}
bmr <- with_progress(benchmark(design))
```

```{r}
autoplot(bmr, measure = measure)
```

```{r}
autoplot(bmr, measure = measure, type = "roc")
```



```{r}
autoplot(bmr, measure = measure, type = "prc")
```
```{r}
autoplot(bmr, measure = f1)
```


```{r}
autoplot(bmr, measure = precision)
```


```{r}
autoplot(bmr, measure = roc_auc)
```


```{r}
autoplot(bmr, measure = prauc)
```


```{r}
autoplot(bmr, measure = recall)
```
